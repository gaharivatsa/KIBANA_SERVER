# üìä KIBANA LOGS TOOL - AI USAGE SPECIFICATION


## üö® MANDATORY REQUIREMENTS
1. **SETUP DYNAMIC CONFIGURATION (FIRST STEP - MANDATORY IF DEFAULTS ARE NOT SUITABLE)**
   - Before proceeding to any other steps, if the default server configurations are not suitable for the user's environment or task, you MUST use the `/api/set_config` endpoint to adjust them.
   - SAY TO USER WHAT ALL CONFIGS TO SET 
   - This allows tailoring the server's behavior (e.g., target Kibana instance, log processing parameters) without manual `config.yaml` edits or server restarts.
   - **Endpoint:** `POST /api/set_config`
   - **Request Body Example:**
     ```json
     {
       "key_path": "elasticsearch.host",
       "value": "your-company-kibana.example.com"
     }
     ```
   - **Commonly Used Configuration Keys (key_path: value_type - example_value):**
     - `elasticsearch.host`: string - e.g., "kibana.example.com" (Target Kibana host)
     - `elasticsearch.port`: integer - e.g., 443 (Target Kibana port)
     - `elasticsearch.timestamp_field`: string - e.g., "@timestamp", "timestamp", "start_time" (Primary timestamp field in logs)
     - `elasticsearch.verify_ssl`: boolean - e.g., true, false (Whether to verify SSL for Kibana connection)
     - `mcp_server.log_level`: string - e.g., "info", "debug", "error" (Server's own logging verbosity)
     - `processing.max_logs`: integer - e.g., 500 (Default max logs for `search_logs` if not specified in request)
     - `processing.default_time_range`: string - e.g., "1h", "24h", "7d" (Default time range for queries if not specified)
     - `processing.cache_ttl_seconds`: integer - e.g., 300 (Cache duration for repeated queries)
     - `processing.timeout_seconds`: integer - e.g., 60 (Timeout for Kibana queries)
     - `ai_response_settings.temperature`: float - e.g., 0.3 (Controls AI creativity, lower is more factual)
     - `ai_response_settings.max_tokens`: integer - e.g., 2000000 (Max tokens for AI generated responses)
     - `ai_response_settings.detailed_mode`: boolean - e.g., true (Enable more detailed AI responses)
     - `ai_providers.google_ai_api_key`: string - e.g., "AIza..." (Set Google AI API key; **Caution: handle API keys securely**)
     - `ai_providers.openai_api_key`: string - e.g., "sk-..." (Set OpenAI API key; **Caution: handle API keys securely**)
   - **Note:** Only set these if you understand their impact or if instructed by the user. Incorrect values might lead to errors. The server will use values from `config.yaml` if not dynamically overridden.
2. **Set auth token first** before using any other endpoint
3. **NEXT ask user which index to use** - after setting auth token but before asking for session ID
   - If user asks to "get index" or "show indexes", call discover_indexes
   - Have user select an index using set_current_index before proceeding
4. **ALWAYS request `session_id` from the user , if not given ask whether can i use order id to get id and proceed . if yes then do so ** - this is required for all queries
5. **ALWAYS include `session_id` in KQL queries** using format: `"{session_id} AND additional_query"`
6. **ALWAYS use Kibana Query Language (KQL)** for all query formatting
7. **ALWAYS use ISO format timestamps** (e.g., "2023-06-15T00:00:00Z") instead of relative time strings
8. **Parse and present results clearly** in a structured format
9. **dont use summarize_logs by yourself , use only when user asks for log summary  . 
10. **Detect and use the right index** - allow users to discover and select the appropriate index for their company



How to Perform Root Cause Analysis (RCA)
Start with the Session ID:
Begin the RCA process immediately after obtaining the session_id.

Initial Log Extraction:
Fetch the first 200 logs in ascending order using only the session_id as the query. This sets the baseline timeline.

Understand the Anomaly:
Analyze the user-reported issue and begin identifying patterns or anomalies in the initial log batch.

Craft Intelligent Queries:
Use insights from the codebase to construct advanced, context-aware queries that help narrow down potential causes.

Correlate Logs with Code:
As logs are fetched, map them to related functions, services, and error flows in the codebase to deepen understanding.

Iterate & Converge:
Refine your queries recursively, incorporating knowledge from previous logs and the codebase. Keep digging until you clearly identify the root cause.

Optimize Query Strategy:
‚ö†Ô∏è Always choose the most effective query for faster convergence. Maintain clarity and precision at every step‚Äîaccuracy is non-negotiable.


## üîë AUTHENTICATION SETUP (REQUIRED FIRST STEP)
```bash
curl -X POST http://localhost:8000/api/set_auth_token \
  -H "Content-Type: application/json" \
  -d '{"auth_token":"YOUR_AUTH_TOKEN_HERE"}'
```

Example:
```bash
curl -X POST http://localhost:8000/api/set_auth_token -H "Content-Type: application/json" -d '{"auth_token":"asdahduwdnsaiuefehasadflhy"}'
```

## üóÇÔ∏è INDEX SELECTION (SECOND STEP FOR NEW USERS)

1. **Discover available indexes:**
   ```bash
   curl -X GET http://localhost:8000/api/discover_indexes
   ```

2. **Set the index to use:**
   ```bash
   curl -X POST http://localhost:8000/api/set_current_index \
     -H "Content-Type: application/json" \
     -d '{"index_pattern": "YOUR_INDEX_PATTERN"}'
   ```

   Example:
   ```bash
   curl -X POST http://localhost:8000/api/set_current_index \
     -H "Content-Type: application/json" \
     -d '{"index_pattern": "estio-logs-*"}'
   ```

3. **When a user says they're from a specific company:**
   - First discover available indexes
   - Look for index patterns that might match their company name
   - Suggest setting the index that seems most appropriate
   - If unsure, ask the user to select from the available indexes
   
4. **Index preference persistence:**
   - Once set, the index preference will persist until changed
   - If a user switches companies or needs a different index, use set_current_index again


To extract the **Session ID** using the **Order ID** from Kibana logs during the `callStartPayment` phase.

### üîç Step-by-Step Rule:

1. **Use the extract_session_id Endpoint (Preferred Method):**

   ```bash
   curl -X POST http://localhost:8000/api/extract_session_id \
     -H "Content-Type: application/json" \
     -d '{
       "order_id": "YOUR_ORDER_ID_HERE"
     }'
   ```

   * This endpoint uses Neurolink AI to automatically extract session IDs
   * It follows all the rules below and returns structured results

2. **Fallback Method (If extract_session_id Fails):**
   
   Use the search_logs endpoint with this query:

   ```kibana
   {Order ID} AND "callStartPayment"
   ```

   * Replace `{Order ID}` with the actual order ID.
   * Use Kibana Query Language (KQL) in the query box.
   * max_results must always be 1-3

3. **Analyze Response Logs (For Fallback Method):**

   * Look for log lines that start with this pattern:

     ```
     message:{num} | {some_id} | {session_id} |
     ```

     * The line always starts with a prefix like `message:`num followed by two pipe-separated values.
     * The **third segment** (after the second `|`) is the **Session ID**.
     * max_results must always be 1-3

4. **IMPORTANT: If the extract_session_id endpoint fails, you MUST use the fallback method with search_logs**
   * Parse the logs manually using the pattern above
   * Extract the session ID from the third segment after the second pipe character
   * Proceed with the extracted session ID for subsequent queries

---

** Function based mode Begin  ** 

this will be triggered by user saying search based on functions

- if this mode is used : all your query text must have {session_id} AND ("FunctionCallResult" OR "FunctionCalled") + query terms 

- always use sort by timestamp and sort order asc in this mode

** Function based mode End. **
## üìã API ENDPOINTS REFERENCE

| Endpoint | Primary Use Case | Key Parameters |
|----------|------------------|----------------|
| `/api/search_logs` | **MAIN ENDPOINT** - Find specific logs | `query_text`, `max_results`, time filters |
| `/api/extract_session_id` | **EXTRACT SESSION ID** - Get session ID from order ID | `order_id` |
| `/api/get_recent_logs` | View latest logs | `count`, `level` |
| `/api/analyze_logs` | Identify patterns | `time_range`, `group_by` |
| `/api/extract_errors` | Find errors | `hours`, `include_stack_traces`, `limit` |

| `/api/summarize_logs` | üß† **AI-POWERED** - Generate intelligent log analysis | Same as `search_logs` + AI analysis |
| `/api/discover_indexes` | List available indexes | None |
| `/api/set_current_index` | Select index for searches | `index_pattern` |

## üîé SEARCH_LOGS ENDPOINT - DETAILED SPECIFICATION

### Required Parameters:
- `query_text`: String using KQL format - MUST include `{session_id}` + query terms
- `max_results`: Integer (default: 100) - Number of results to return (adjust to your needs)
### Optional Parameters:
- `start_time`/`end_time`: ISO timestamps (e.g., "2023-06-15T00:00:00Z")
- `levels`: Array of log levels to include (e.g., ["ERROR", "WARN"])
- `include_fields`/`exclude_fields`: Arrays of field names
- `sort_by`: Field to sort by - MUST use a valid timestamp field name (`timestamp`, `@timestamp`, or `start_time`)
- `sort_order`: "asc" or "desc" (default: "desc")

### KQL Query Examples:
```
"{session_id} AND error"
"{session_id} AND payment"
"{session_id} AND verifyPaymentAttempt"
"a71e84b1ee00f6247347 AND callStartPayment"
```

### Complete Examples:

#### 1. Basic Search with Session ID
```bash
curl -X POST http://localhost:8000/api/search_logs \
  -H "Content-Type: application/json" \
  -d '{
    "query_text": "{session_id} AND payment error",
    "max_results": 50
  }'
```

#### 2. Search with Absolute Time Range
```bash
curl -X POST http://localhost:8000/api/search_logs \
  -H "Content-Type: application/json" \
  -d '{
    "query_text": "{session_id} AND payment",
    "start_time": "2023-06-15T00:00:00Z",
    "end_time": "2023-06-16T00:00:00Z"
  }'
```

#### 3. Search with Specific Transaction
```bash
curl -X POST http://localhost:8000/api/search_logs \
  -H "Content-Type: application/json" \
  -d '{
    "query_text": "{session_id} AND verifyPaymentAttempt",
    "max_results": 1
  }'
```

#### 4. Search with Correct Sorting
```bash
curl -X POST http://localhost:8000/api/search_logs \
  -H "Content-Type: application/json" \
  -d '{
    "query_text": "{session_id} AND payment",
    "sort_by": "timestamp",
    "sort_order": "desc"
  }'
```

#### 5. Extract Session ID from Order ID
```bash
curl -X POST http://localhost:8000/api/extract_session_id \
  -H "Content-Type: application/json" \
  -d '{
    "order_id": "ORDER_12345"
  }'
```

## üìä OTHER ENDPOINTS - REFERENCE EXAMPLES

### Analyze Logs for Patterns
```bash
curl -X POST http://localhost:8000/api/analyze_logs \
  -H "Content-Type: application/json" \
  -d '{
    "time_range": "24h",
    "group_by": "level"
  }'
```

### Fetch Recent Logs
```bash
curl -X POST http://localhost:8000/api/get_recent_logs \
  -H "Content-Type: application/json" \
  -d '{
    "count": 10,
    "level": "ERROR"
  }'
```



!CAUTION DONT USE THIS ENDPOINT WITHOUT USER EXPLICITLY ASKING FOR IT (summarize_logs)
### üß† AI-Powered Log Analysis (SUMMARIZE_LOGS)
```bash
curl -X POST http://localhost:8000/api/summarize_logs \
  -H "Content-Type: application/json" \
  -d '{
    "query_text": "{session_id} AND {other query}",
    "max_results": 50,
    "start_time": "2023-06-15T00:00:00Z",
    "end_time": "2023-06-16T00:00:00Z"
  }'
```

**What it does:**
- Searches logs using the same parameters as `search_logs`
- Uses AI (Neurolink) to generate intelligent analysis
- Provides structured insights including:
  - Summary of activities and systems involved
  - Key insights and patterns
  - Errors and exceptions with details
  - Function calls and critical methods
  - Chronological flow of events
  - Anomalies and suspicious behavior
  - Focus areas needing attention
  - Recommendations for fixes

**When to use:**
- When you need to understand complex log patterns
- For root cause analysis of issues
- When dealing with large volumes of logs
- For generating reports or summaries
- When you need actionable insights from logs

**Requirements:**
- Requires Node.js and Neurolink (automatically set up when starting server)
- Requires AI provider API key for full AI features. 
  Set via environment variable (e.g., `export GOOGLE_AI_API_KEY=...`) 
  OR in `config.yaml` under `ai_providers` (config values take precedence).
  Google AI Studio key is recommended (has a free tier).
- May take longer than regular search due to AI processing

## üîÑ AI DECISION FLOW

1. **Set auth token** (if not already set)
2. **Ask which index to use** and call discover_indexes if needed
3. **Request `session_id` from user** (if not provided)
4. **Determine appropriate endpoint** based on user intent:
   - For specific searches ‚Üí `search_logs`
   - For recent errors ‚Üí `get_recent_logs` or `extract_errors`
   - For pattern analysis ‚Üí `analyze_logs`
   - **For intelligent analysis/insights ‚Üí `summarize_logs` (AI-powered)**
4. **Construct query** using KQL format: `"{session_id} AND <query terms>"`
5. **Select appropriate parameters** (time range, result limits, etc.)
6. **Execute request** using the curl command
7. **Parse and present results** in a clear, structured format

## üìå IMPORTANT NOTES

- Always replace `{session_id}` with the actual session ID in queries
- Always use ISO format timestamps (e.g., "2023-06-15T00:00:00Z") instead of relative time strings
- Include relevant context in queries to narrow results
- Use field filters (`include_fields` and `exclude_fields`) for large result sets
- Set appropriate `max_results` to balance detail vs. performance

## ‚ö†Ô∏è TROUBLESHOOTING COMMON ERRORS

### Timestamp Field Issues
- When using `sort_by`, you MUST use a valid timestamp field that exists in the indices
- Different indices use different field names for timestamps:
  - `timestamp` - Used in breeze-logs source
  - `@timestamp` - Standard Elasticsearch default
  - `start_time` - Used in main elasticsearch config
- If you see "No mapping found for [field] in order to sort on" error:
  1. Try using a different timestamp field name (`timestamp`, `@timestamp`, or `start_time`)
  2. The server will automatically retry without sorting if it encounters this error
  3. You can omit the `sort_by` parameter to let the server use its default sorting logic

### Example with Correct Timestamp Field
```bash
# Correct - Using a valid timestamp field:
curl -X POST http://localhost:8000/api/search_logs \
  -H "Content-Type: application/json" \
  -d '{
    "query_text": "{session_id} AND error",
    "sort_by": "timestamp",
    "sort_order": "desc"
  }'
```




